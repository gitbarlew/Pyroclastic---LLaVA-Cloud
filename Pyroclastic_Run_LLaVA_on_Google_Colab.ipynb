{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNtudeIn/YXhQ6oyf5pOF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitbarlew/Pyroclastic---LLaVA-Cloud/blob/main/Pyroclastic_Run_LLaVA_on_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyroclastic - Run LLaVa instance in the cloud with localtunnel\n",
        "\n",
        "Below code downloads, installs and runs Ollama instance. Further steps allow to expose this instance to the public internet using localtunnel.  \n",
        "\n"
      ],
      "metadata": {
        "id": "CsSsOhDUWHzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pull Ollama install script and execute\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "#Run Ollama in the background\n",
        "!ollama serve &>/dev/null&\n",
        "#Download LLaVA model (13b, 1.6ver)\n",
        "!ollama pull llava:13b-v1.6"
      ],
      "metadata": {
        "id": "00mBH_2zWGwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check public IP address of the Google Colab instance. This will be used later as a password for localtunnel protection check using browser to check service availability.  "
      ],
      "metadata": {
        "id": "7U2Yrr1EpIA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ifconfig.me"
      ],
      "metadata": {
        "id": "fsGl8RHdZmzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's install and run localtunnel, exposing Ollamas' default API port 11434."
      ],
      "metadata": {
        "id": "HhIMV6g0WRUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and run the Ollama Linux install script\n",
        "!npm install localtunnel\n",
        "!npx localtunnel --port 11434"
      ],
      "metadata": {
        "id": "CfPs3PpcWN0t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}